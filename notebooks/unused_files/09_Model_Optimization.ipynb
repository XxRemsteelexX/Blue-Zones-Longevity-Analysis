{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization and Feature Analysis\n",
    "\n",
    "This notebook systematically analyzes all features to identify what factors truly influence longevity, optimizes predictive models, and creates actionable insights.\n",
    "\n",
    "## Analysis Components\n",
    "\n",
    "1. Comprehensive correlation analysis of all features\n",
    "2. Statistical visualization of significant relationships\n",
    "3. Multiple model comparison and optimization\n",
    "4. Feature importance ranking and interpretation\n",
    "5. Actionable policy recommendations\n",
    "6. Blue Zone scoring system development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "# Suppress specific warnings only when necessary\n",
    "# warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('default')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "# Set light blue background for plots\n",
    "plt.rcParams['axes.facecolor'] = '#E5ECF6'\n",
    "\n",
    "print(\"Model optimization notebook initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_all_data():\n    \"\"\"\n    Load all available real data sources and combine them\n    \"\"\"\n    # Define real data files only\n    potential_files = [\n        '../outputs/cross_section_final.csv',\n        '../outputs/final_processed_data.csv',\n        '../outputs/comprehensive_panel_data.csv'\n    ]\n    \n    loaded_files = []\n    data_sources = []\n    \n    # Try to load each real data file\n    for filepath in potential_files:\n        try:\n            if os.path.exists(filepath):\n                df = pd.read_csv(filepath)\n                if 'life_expectancy' in df.columns and len(df) > 0:\n                    data_sources.append(df)\n                    loaded_files.append(filepath)\n                    print(f\"Loaded {filepath.split('/')[-1]}: {len(df)} rows, {len(df.columns)} columns\")\n        except Exception as e:\n            print(f\"Could not load {filepath}: {e}\")\n    \n    if not data_sources:\n        raise FileNotFoundError(\"No real data files found. Run the Python analysis scripts first to generate data.\")\n    \n    # Use the largest/most complete dataset\n    df = max(data_sources, key=len)\n    print(f\"\\nUsing dataset with {len(df)} observations\")\n    \n    # Basic data cleaning\n    df = df.dropna(subset=['life_expectancy'])\n    \n    print(f\"Final dataset: {len(df)} observations\")\n    print(f\"Total columns: {len(df.columns)}\")\n    \n    return df\n\n# Load the data\ndf = load_all_data()\n\nif not df.empty:\n    print(f\"\\nData summary:\")\n    print(f\"Life expectancy range: {df['life_expectancy'].min():.1f} - {df['life_expectancy'].max():.1f} years\")\n    print(f\"Blue Zone regions: {df['is_blue_zone'].sum() if 'is_blue_zone' in df.columns else 'Not available'}\")\nelse:\n    print(\"Warning: No data loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_correlation_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyze correlations between all features and life expectancy\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data available for correlation analysis\")\n",
    "        return []\n",
    "    \n",
    "    print(\"Comprehensive Correlation Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get all numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove non-feature columns\n",
    "    exclude = ['year', 'is_blue_zone']\n",
    "    feature_cols = [col for col in numeric_cols if col not in exclude and col != 'life_expectancy']\n",
    "    \n",
    "    print(f\"Analyzing {len(feature_cols)} features for correlations with life expectancy\")\n",
    "    \n",
    "    # Calculate correlations and p-values\n",
    "    correlations = {}\n",
    "    p_values = {}\n",
    "    sample_sizes = {}\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        # Clean data - remove missing values\n",
    "        mask = df[col].notna() & df['life_expectancy'].notna()\n",
    "        valid_data = mask.sum()\n",
    "        \n",
    "        if valid_data > 3:  # Need at least 4 points for correlation\n",
    "            try:\n",
    "                corr, p_val = stats.pearsonr(df[mask][col], df[mask]['life_expectancy'])\n",
    "                correlations[col] = corr\n",
    "                p_values[col] = p_val\n",
    "                sample_sizes[col] = valid_data\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating correlation for {col}: {e}\")\n",
    "    \n",
    "    # Sort by absolute correlation strength\n",
    "    sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nTop correlations with life expectancy:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Feature':<35} {'Correlation':>12} {'P-Value':>12} {'N':>8} {'Significance':>12}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    significant_features = []\n",
    "    \n",
    "    for feature, corr in sorted_correlations[:25]:  # Show top 25\n",
    "        p_val = p_values[feature]\n",
    "        n = sample_sizes[feature]\n",
    "        \n",
    "        # Determine significance level\n",
    "        if p_val < 0.001:\n",
    "            sig = \"***\"\n",
    "        elif p_val < 0.01:\n",
    "            sig = \"**\"\n",
    "        elif p_val < 0.05:\n",
    "            sig = \"*\"\n",
    "        elif p_val < 0.1:\n",
    "            sig = \".\"\n",
    "        else:\n",
    "            sig = \"\"\n",
    "        \n",
    "        print(f\"{feature:<35} {corr:>12.4f} {p_val:>12.4f} {n:>8} {sig:>12}\")\n",
    "        \n",
    "        # Keep significant features (p < 0.1 for exploratory analysis)\n",
    "        if p_val < 0.1:\n",
    "            significant_features.append((feature, corr, p_val))\n",
    "    \n",
    "    print(\"\\nSignificance codes: *** p<0.001, ** p<0.01, * p<0.05, . p<0.1\")\n",
    "    print(f\"\\nFound {len(significant_features)} significant features (p < 0.1)\")\n",
    "    \n",
    "    return significant_features\n",
    "\n",
    "# Perform correlation analysis\n",
    "if not df.empty:\n",
    "    significant_features = comprehensive_correlation_analysis(df)\n",
    "else:\n",
    "    significant_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_visualizations(df, significant_features):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of significant features\n",
    "    \"\"\"\n",
    "    if len(significant_features) == 0:\n",
    "        print(\"No significant features to visualize\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nCreating visualizations for top {min(8, len(significant_features))} features...\")\n",
    "    \n",
    "    # Select top features\n",
    "    top_features = significant_features[:8]\n",
    "    \n",
    "    # Create subplot layout\n",
    "    n_features = len(top_features)\n",
    "    n_cols = 4\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))\n",
    "    if n_features == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if n_rows > 1 else axes\n",
    "    \n",
    "    for i, (feature, corr, p_val) in enumerate(top_features):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Clean data for this feature\n",
    "        mask = df[feature].notna() & df['life_expectancy'].notna()\n",
    "        if mask.sum() == 0:\n",
    "            ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "            continue\n",
    "        \n",
    "        x = df[mask][feature]\n",
    "        y = df[mask]['life_expectancy']\n",
    "        \n",
    "        # Determine colors based on Blue Zone status if available\n",
    "        if 'is_blue_zone' in df.columns:\n",
    "            colors = ['red' if bz == 1 else 'steelblue' for bz in df[mask]['is_blue_zone']]\n",
    "            alpha = 0.7\n",
    "        else:\n",
    "            colors = 'steelblue'\n",
    "            alpha = 0.6\n",
    "        \n",
    "        # Create scatter plot\n",
    "        ax.scatter(x, y, c=colors, alpha=alpha, s=50, edgecolors='white', linewidth=0.5)\n",
    "        \n",
    "        # Add trend line\n",
    "        try:\n",
    "            z = np.polyfit(x, y, 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_line = np.linspace(x.min(), x.max(), 100)\n",
    "            ax.plot(x_line, p(x_line), color='darkgreen', linestyle='--', alpha=0.8, linewidth=2)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Formatting\n",
    "        feature_name = feature.replace('_', ' ').title()\n",
    "        ax.set_xlabel(feature_name, fontsize=10)\n",
    "        ax.set_ylabel('Life Expectancy (years)', fontsize=10)\n",
    "        ax.set_title(f'r = {corr:.3f}, p = {p_val:.3f}', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Add R-squared in corner\n",
    "        r2 = corr ** 2\n",
    "        ax.text(0.05, 0.95, f'R² = {r2:.3f}', transform=ax.transAxes, \n",
    "                verticalalignment='top', fontsize=9, \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    plt.suptitle('Most Significant Features for Longevity Prediction', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    import os\n",
    "    output_dir = '../outputs/figures'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'significant_features_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    print(f\"Feature visualization saved to {output_dir}/significant_features_analysis.png\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create feature visualizations\n",
    "if significant_features and not df.empty:\n",
    "    create_feature_visualizations(df, significant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_heatmap(df, features):\n",
    "    \"\"\"\n",
    "    Create correlation heatmap for top features\n",
    "    \"\"\"\n",
    "    if len(features) == 0:\n",
    "        print(\"No features available for heatmap\")\n",
    "        return\n",
    "    \n",
    "    # Prepare feature list\n",
    "    feature_names = [f[0] for f in features[:12]]  # Top 12 features\n",
    "    feature_subset = [f for f in feature_names if f in df.columns] + ['life_expectancy']\n",
    "    \n",
    "    # Create clean dataset\n",
    "    clean_df = df[feature_subset].dropna()\n",
    "    \n",
    "    if len(clean_df) < 5:\n",
    "        print(\"Insufficient data for correlation heatmap\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Creating correlation heatmap with {len(feature_subset)} features...\")\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = clean_df.corr()\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create mask for upper triangle to show only lower triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Generate heatmap\n",
    "    sns.heatmap(corr_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='RdBu_r', \n",
    "                center=0,\n",
    "                fmt='.3f', \n",
    "                square=True, \n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation Coefficient\"},\n",
    "                annot_kws={'size': 8})\n",
    "    \n",
    "    plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    import os\n",
    "    output_dir = '../outputs/figures'\n",
    "    plt.savefig(os.path.join(output_dir, 'correlation_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "    print(f\"Correlation heatmap saved to {output_dir}/correlation_heatmap.png\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create correlation heatmap\n",
    "if significant_features and not df.empty:\n",
    "    create_correlation_heatmap(df, significant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_optimize_models(df, significant_features, max_features=15):\n",
    "    \"\"\"\n",
    "    Build and optimize multiple prediction models\n",
    "    \"\"\"\n",
    "    if len(significant_features) == 0:\n",
    "        print(\"No significant features available for modeling\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(\"Building and Optimizing Prediction Models\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_names = [f[0] for f in significant_features[:max_features]]\n",
    "    available_features = [f for f in feature_names if f in df.columns]\n",
    "    \n",
    "    # Create clean dataset\n",
    "    model_data = df[available_features + ['life_expectancy']].dropna()\n",
    "    \n",
    "    if len(model_data) < 10:\n",
    "        print(f\"Insufficient data for modeling: only {len(model_data)} complete observations\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"Using {len(model_data)} observations with {len(available_features)} features\")\n",
    "    print(f\"Features: {', '.join(available_features)}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = model_data[available_features]\n",
    "    y = model_data['life_expectancy']\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Define models with hyperparameter grids\n",
    "    models = {\n",
    "        'Linear Regression': {\n",
    "            'model': LinearRegression(),\n",
    "            'params': {}\n",
    "        },\n",
    "        'Ridge Regression': {\n",
    "            'model': Ridge(),\n",
    "            'params': {'alpha': [0.1, 1.0, 10.0, 100.0]}\n",
    "        },\n",
    "        'Elastic Net': {\n",
    "            'model': ElasticNet(),\n",
    "            'params': {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 0.9]}\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 5, 10, None],\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "            }\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.01, 0.1, 0.2]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    cv_folds = min(5, len(model_data))  # Use fewer folds for small datasets\n",
    "    kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    results = {}\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_model_name = None\n",
    "    \n",
    "    print(f\"\\nModel Evaluation ({cv_folds}-fold cross-validation):\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Model':<20} {'CV R²':>10} {'Std':>8} {'Train R²':>10} {'MAE':>8} {'RMSE':>8}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for name, model_info in models.items():\n",
    "        model = model_info['model']\n",
    "        param_grid = model_info['params']\n",
    "        \n",
    "        try:\n",
    "            # Grid search for hyperparameter optimization\n",
    "            if param_grid:\n",
    "                # Limit grid search for small datasets\n",
    "                inner_cv = min(3, len(model_data))\n",
    "                grid_search = GridSearchCV(\n",
    "                    model, param_grid, \n",
    "                    cv=inner_cv, \n",
    "                    scoring='r2', \n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                grid_search.fit(X_scaled, y)\n",
    "                optimized_model = grid_search.best_estimator_\n",
    "            else:\n",
    "                optimized_model = model\n",
    "                optimized_model.fit(X_scaled, y)\n",
    "            \n",
    "            # Cross-validation scores\n",
    "            cv_scores = cross_val_score(optimized_model, X_scaled, y, cv=kfold, scoring='r2')\n",
    "            cv_mean = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "            # Training performance\n",
    "            y_pred = optimized_model.predict(X_scaled)\n",
    "            train_r2 = r2_score(y, y_pred)\n",
    "            mae = mean_absolute_error(y, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': optimized_model,\n",
    "                'cv_r2_mean': cv_mean,\n",
    "                'cv_r2_std': cv_std,\n",
    "                'train_r2': train_r2,\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'best_params': getattr(grid_search, 'best_params_', {}) if param_grid else {}\n",
    "            }\n",
    "            \n",
    "            print(f\"{name:<20} {cv_mean:>10.4f} {cv_std:>8.4f} {train_r2:>10.4f} {mae:>8.2f} {rmse:>8.2f}\")\n",
    "            \n",
    "            # Track best model\n",
    "            if cv_mean > best_score:\n",
    "                best_score = cv_mean\n",
    "                best_model = optimized_model\n",
    "                best_model_name = name\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"{name:<20} ERROR: {str(e)[:30]}...\")\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model_name} (CV R² = {best_score:.4f})\")\n",
    "    \n",
    "    return best_model, scaler, available_features, results\n",
    "\n",
    "# Build and optimize models\n",
    "if significant_features and not df.empty:\n",
    "    model_results = build_and_optimize_models(df, significant_features)\n",
    "    best_model, scaler, model_features, all_results = model_results if model_results[0] is not None else (None, None, None, None)\n",
    "else:\n",
    "    best_model, scaler, model_features, all_results = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(best_model, model_features, model_name):\n",
    "    \"\"\"\n",
    "    Analyze and visualize feature importance from the best model\n",
    "    \"\"\"\n",
    "    if best_model is None or model_features is None:\n",
    "        print(\"No model available for feature importance analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nFeature Importance Analysis ({model_name})\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Extract feature importance based on model type\n",
    "    importance_data = None\n",
    "    \n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        # Tree-based models\n",
    "        importance_data = pd.DataFrame({\n",
    "            'feature': model_features,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"Feature Importance (from tree-based model):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    elif hasattr(best_model, 'coef_'):\n",
    "        # Linear models\n",
    "        coefficients = best_model.coef_\n",
    "        importance_data = pd.DataFrame({\n",
    "            'feature': model_features,\n",
    "            'coefficient': coefficients,\n",
    "            'abs_coefficient': np.abs(coefficients)\n",
    "        }).sort_values('abs_coefficient', ascending=False)\n",
    "        \n",
    "        print(\"Feature Coefficients (from linear model):\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    if importance_data is not None:\n",
    "        # Display importance table\n",
    "        for i, (_, row) in enumerate(importance_data.head(15).iterrows(), 1):\n",
    "            if 'importance' in row:\n",
    "                print(f\"{i:2d}. {row['feature']:<30} {row['importance']:>8.4f}\")\n",
    "            else:\n",
    "                direction = '+' if row['coefficient'] > 0 else '-'\n",
    "                print(f\"{i:2d}. {row['feature']:<30} {row['coefficient']:>8.4f} ({direction})\")\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        if 'importance' in importance_data.columns:\n",
    "            # Feature importance bar plot\n",
    "            top_features = importance_data.head(12)\n",
    "            plt.barh(range(len(top_features)), top_features['importance'])\n",
    "            plt.yticks(range(len(top_features)), [f.replace('_', ' ').title() for f in top_features['feature']])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title(f'Feature Importance - {model_name}')\n",
    "        else:\n",
    "            # Coefficient plot\n",
    "            top_features = importance_data.head(12)\n",
    "            colors = ['red' if c < 0 else 'blue' for c in top_features['coefficient']]\n",
    "            plt.barh(range(len(top_features)), top_features['coefficient'], color=colors)\n",
    "            plt.yticks(range(len(top_features)), [f.replace('_', ' ').title() for f in top_features['feature']])\n",
    "            plt.xlabel('Coefficient Value')\n",
    "            plt.title(f'Feature Coefficients - {model_name}')\n",
    "            plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        import os\n",
    "        output_dir = '../outputs/figures'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nFeature importance plot saved to {output_dir}/feature_importance.png\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return importance_data\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Analyze feature importance\n",
    "importance_data = None\n",
    "if best_model is not None:\n",
    "    # Find the best model name\n",
    "    best_model_name = None\n",
    "    if all_results:\n",
    "        best_score = -np.inf\n",
    "        for name, results in all_results.items():\n",
    "            if results['cv_r2_mean'] > best_score:\n",
    "                best_score = results['cv_r2_mean']\n",
    "                best_model_name = name\n",
    "    \n",
    "    importance_data = analyze_feature_importance(best_model, model_features, best_model_name or 'Best Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Tool Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_tool(model, scaler, features):\n",
    "    \"\"\"\n",
    "    Create a practical prediction tool for life expectancy\n",
    "    \"\"\"\n",
    "    if model is None or scaler is None or features is None:\n",
    "        print(\"Cannot create prediction tool: model components missing\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Creating Life Expectancy Prediction Tool\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    def predict_life_expectancy(**kwargs):\n",
    "        \"\"\"\n",
    "        Predict life expectancy based on input features\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Feature values as keyword arguments\n",
    "            \n",
    "        Returns:\n",
    "            float: Predicted life expectancy in years\n",
    "        \"\"\"\n",
    "        # Prepare input vector\n",
    "        input_vector = []\n",
    "        missing_features = []\n",
    "        \n",
    "        for feature in features:\n",
    "            if feature in kwargs:\n",
    "                input_vector.append(kwargs[feature])\n",
    "            else:\n",
    "                input_vector.append(0)  # Use 0 (scaled mean) for missing features\n",
    "                missing_features.append(feature)\n",
    "        \n",
    "        # Convert to numpy array and reshape\n",
    "        input_array = np.array(input_vector).reshape(1, -1)\n",
    "        \n",
    "        # Scale the input\n",
    "        input_scaled = scaler.transform(input_array)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(input_scaled)[0]\n",
    "        \n",
    "        return prediction, missing_features\n",
    "    \n",
    "    # Test the prediction tool\n",
    "    print(\"\\nTesting Prediction Tool:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create realistic test cases\n",
    "    test_scenarios = {\n",
    "        'High Development': {\n",
    "            'gdp_per_capita': 50000,\n",
    "            'health_exp_per_capita': 5000,\n",
    "            'urban_pop_pct': 85,\n",
    "            'physicians_per_1000': 4.0,\n",
    "            'forest_area_pct': 35\n",
    "        },\n",
    "        'Middle Development': {\n",
    "            'gdp_per_capita': 15000,\n",
    "            'health_exp_per_capita': 800,\n",
    "            'urban_pop_pct': 65,\n",
    "            'physicians_per_1000': 2.0,\n",
    "            'forest_area_pct': 25\n",
    "        },\n",
    "        'Low Development': {\n",
    "            'gdp_per_capita': 3000,\n",
    "            'health_exp_per_capita': 100,\n",
    "            'urban_pop_pct': 35,\n",
    "            'physicians_per_1000': 0.5,\n",
    "            'forest_area_pct': 15\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for scenario_name, inputs in test_scenarios.items():\n",
    "        try:\n",
    "            prediction, missing = predict_life_expectancy(**inputs)\n",
    "            print(f\"{scenario_name:<20}: {prediction:.1f} years\")\n",
    "            \n",
    "            # Show key inputs\n",
    "            key_features = ['gdp_per_capita', 'health_exp_per_capita', 'urban_pop_pct']\n",
    "            input_summary = ', '.join([f\"{k.replace('_', ' ').title()}: {inputs.get(k, 'N/A')}\"\n",
    "                                     for k in key_features if k in inputs])\n",
    "            print(f\"{'':20}  ({input_summary})\")\n",
    "        except Exception as e:\n",
    "            print(f\"{scenario_name:<20}: Error - {e}\")\n",
    "    \n",
    "    print(f\"\\nPrediction tool created successfully!\")\n",
    "    print(f\"Features used: {', '.join(features)}\")\n",
    "    \n",
    "    return predict_life_expectancy\n",
    "\n",
    "# Create prediction tool\n",
    "prediction_tool = None\n",
    "if best_model is not None:\n",
    "    prediction_tool = create_prediction_tool(best_model, scaler, model_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-Actionable Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_actionable_features(significant_features):\n",
    "    \"\"\"\n",
    "    Categorize features by their policy actionability\n",
    "    \"\"\"\n",
    "    if not significant_features:\n",
    "        print(\"No significant features to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"Policy-Actionable Features Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Categorize features by actionability\n",
    "    feature_categories = {\n",
    "        'Economic Policy': {\n",
    "            'features': ['gdp_per_capita', 'health_exp_per_capita', 'education_expenditure'],\n",
    "            'description': 'Government economic and fiscal policies'\n",
    "        },\n",
    "        'Healthcare System': {\n",
    "            'features': ['physicians_per_1000', 'hospital_beds_per_1000', 'health_exp_per_capita', 'mortality_rate'],\n",
    "            'description': 'Healthcare infrastructure and access'\n",
    "        },\n",
    "        'Environmental Policy': {\n",
    "            'features': ['forest_area_pct', 'co2_emissions', 'pm25', 'air_quality'],\n",
    "            'description': 'Environmental quality and regulations'\n",
    "        },\n",
    "        'Urban Planning': {\n",
    "            'features': ['urban_pop_pct', 'population_density', 'infrastructure'],\n",
    "            'description': 'City planning and development'\n",
    "        },\n",
    "        'Social Policy': {\n",
    "            'features': ['education_level', 'social_support', 'inequality'],\n",
    "            'description': 'Social programs and welfare'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Fixed/Non-actionable features\n",
    "    non_actionable = {\n",
    "        'Geographic': ['latitude', 'longitude', 'effective_gravity', 'elevation'],\n",
    "        'Climate': ['temperature_est', 'temperature_mean', 'precipitation', 'climate_zone']\n",
    "    }\n",
    "    \n",
    "    print(\"\\nACTIONABLE FEATURES (Policy Levers):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    actionable_found = False\n",
    "    \n",
    "    for category, info in feature_categories.items():\n",
    "        category_features = []\n",
    "        \n",
    "        for feature, corr, p_val in significant_features:\n",
    "            if any(action_feat in feature.lower() for action_feat in info['features']):\n",
    "                category_features.append((feature, corr, p_val))\n",
    "        \n",
    "        if category_features:\n",
    "            actionable_found = True\n",
    "            print(f\"\\n{category.upper()}:\")\n",
    "            print(f\"Policy Area: {info['description']}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for feature, corr, p_val in category_features:\n",
    "                direction = \"increase\" if corr > 0 else \"reduce\"\n",
    "                strength = \"Strong\" if abs(corr) > 0.5 else \"Moderate\" if abs(corr) > 0.3 else \"Weak\"\n",
    "                \n",
    "                print(f\"  {feature.replace('_', ' ').title()}:\")\n",
    "                print(f\"    Action: {direction.title()} to improve longevity\")\n",
    "                print(f\"    Effect Size: {strength} (r = {corr:.3f}, p = {p_val:.3f})\")\n",
    "                print()\n",
    "    \n",
    "    if not actionable_found:\n",
    "        print(\"No clearly actionable features found in top correlations.\")\n",
    "    \n",
    "    # Show non-actionable features for context\n",
    "    print(\"\\nNON-ACTIONABLE FEATURES (Fixed Factors):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    non_actionable_features = []\n",
    "    for feature, corr, p_val in significant_features:\n",
    "        for category, features in non_actionable.items():\n",
    "            if any(fixed_feat in feature.lower() for fixed_feat in features):\n",
    "                non_actionable_features.append((category, feature, corr))\n",
    "                break\n",
    "    \n",
    "    if non_actionable_features:\n",
    "        for category, feature, corr in non_actionable_features:\n",
    "            print(f\"  {feature.replace('_', ' ').title()} ({category}): r = {corr:.3f}\")\n",
    "    \n",
    "    # Policy recommendations\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"KEY POLICY RECOMMENDATIONS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    recommendations = [\n",
    "        \"1. Invest in healthcare infrastructure and access\",\n",
    "        \"2. Improve economic conditions through sustainable development\",\n",
    "        \"3. Implement environmental protection and air quality measures\",\n",
    "        \"4. Focus on education and social program development\",\n",
    "        \"5. Consider geographic and climate constraints in planning\"\n",
    "    ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "\n",
    "# Analyze actionable features\n",
    "if significant_features:\n",
    "    identify_actionable_features(significant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blue Zone Scoring System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blue_zone_scorer(df, significant_features):\n",
    "    \"\"\"\n",
    "    Create a comprehensive Blue Zone scoring system\n",
    "    \"\"\"\n",
    "    if df.empty or not significant_features:\n",
    "        print(\"Insufficient data for Blue Zone scoring system\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Blue Zone Scoring System Development\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define scoring criteria based on analysis results\n",
    "    # Use optimal ranges derived from the data analysis\n",
    "    scoring_criteria = {}\n",
    "    \n",
    "    # Analyze Blue Zones if available to set criteria\n",
    "    if 'is_blue_zone' in df.columns and df['is_blue_zone'].sum() > 0:\n",
    "        blue_zones = df[df['is_blue_zone'] == 1]\n",
    "        print(f\"Analyzing {len(blue_zones)} known Blue Zones for criteria...\")\n",
    "        \n",
    "        # Extract ranges from top significant features\n",
    "        top_features = [f[0] for f in significant_features[:10] if f[0] in blue_zones.columns]\n",
    "        \n",
    "        for feature in top_features:\n",
    "            if blue_zones[feature].notna().sum() > 0:\n",
    "                values = blue_zones[feature].dropna()\n",
    "                if len(values) > 0:\n",
    "                    min_val = values.min()\n",
    "                    max_val = values.max()\n",
    "                    mean_val = values.mean()\n",
    "                    std_val = values.std() if len(values) > 1 else 0\n",
    "                    \n",
    "                    # Create ranges (mean ± 1.5 * std, bounded by min/max)\n",
    "                    optimal_min = max(min_val, mean_val - 1.5 * std_val) if std_val > 0 else min_val\n",
    "                    optimal_max = min(max_val, mean_val + 1.5 * std_val) if std_val > 0 else max_val\n",
    "                    \n",
    "                    # Weight by correlation strength\n",
    "                    corr_strength = abs([f[1] for f in significant_features if f[0] == feature][0])\n",
    "                    weight = max(1, int(corr_strength * 5))  # Scale to 1-5\n",
    "                    \n",
    "                    scoring_criteria[feature] = {\n",
    "                        'range': (optimal_min, optimal_max),\n",
    "                        'weight': weight,\n",
    "                        'correlation': corr_strength\n",
    "                    }\n",
    "    else:\n",
    "        # Use general optimal ranges based on literature and analysis\n",
    "        general_criteria = {\n",
    "            'gdp_per_capita': {'range': (15000, 40000), 'weight': 3},\n",
    "            'health_exp_per_capita': {'range': (800, 3000), 'weight': 3},\n",
    "            'forest_area_pct': {'range': (25, 60), 'weight': 2},\n",
    "            'urban_pop_pct': {'range': (50, 80), 'weight': 2},\n",
    "            'physicians_per_1000': {'range': (2.0, 5.0), 'weight': 2},\n",
    "            'mortality_rate': {'range': (3, 6), 'weight': 1},\n",
    "            'latitude': {'range': (25, 50), 'weight': 1}  # Temperate zones\n",
    "        }\n",
    "        \n",
    "        # Filter to available features\n",
    "        for feature, criteria in general_criteria.items():\n",
    "            if feature in df.columns and any(f[0] == feature for f in significant_features):\n",
    "                scoring_criteria[feature] = criteria\n",
    "    \n",
    "    print(f\"\\nScoring Criteria ({len(scoring_criteria)} features):\")\n",
    "    print(\"-\" * 50)\n",
    "    for feature, criteria in scoring_criteria.items():\n",
    "        min_val, max_val = criteria['range']\n",
    "        weight = criteria['weight']\n",
    "        print(f\"{feature.replace('_', ' ').title():<25}: {min_val:8.1f} - {max_val:8.1f} (weight: {weight})\")\n",
    "    \n",
    "    def calculate_blue_zone_score(row):\n",
    "        \"\"\"\n",
    "        Calculate Blue Zone potential score for a single row\n",
    "        \"\"\"\n",
    "        total_score = 0\n",
    "        max_possible_score = 0\n",
    "        \n",
    "        for feature, criteria in scoring_criteria.items():\n",
    "            if feature in row.index and not pd.isna(row[feature]):\n",
    "                value = row[feature]\n",
    "                min_val, max_val = criteria['range']\n",
    "                weight = criteria['weight']\n",
    "                \n",
    "                # Calculate feature score\n",
    "                if min_val <= value <= max_val:\n",
    "                    # Perfect score if within optimal range\n",
    "                    feature_score = 1.0\n",
    "                else:\n",
    "                    # Partial score based on distance from optimal range\n",
    "                    if value < min_val:\n",
    "                        # Below optimal\n",
    "                        distance = (min_val - value) / (max(min_val, 1))  # Avoid division by zero\n",
    "                    else:\n",
    "                        # Above optimal\n",
    "                        distance = (value - max_val) / (max(max_val, 1))\n",
    "                    \n",
    "                    # Exponential decay for distance penalty\n",
    "                    feature_score = max(0, np.exp(-distance))\n",
    "                \n",
    "                total_score += feature_score * weight\n",
    "                max_possible_score += weight\n",
    "        \n",
    "        # Return score as percentage\n",
    "        return (total_score / max_possible_score * 100) if max_possible_score > 0 else 0\n",
    "    \n",
    "    # Apply scoring to all regions\n",
    "    print(\"\\nCalculating Blue Zone scores...\")\n",
    "    df_scored = df.copy()\n",
    "    df_scored['blue_zone_score'] = df_scored.apply(calculate_blue_zone_score, axis=1)\n",
    "    \n",
    "    # Rank regions by score\n",
    "    ranked_regions = df_scored.sort_values('blue_zone_score', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop Blue Zone Candidates by Score:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Rank':<5} {'Region':<25} {'BZ Score':<10} {'Life Exp':<12} {'Known BZ':<10}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, (_, row) in enumerate(ranked_regions.head(15).iterrows(), 1):\n",
    "        region_name = str(row.get('geo_id', f'Region_{i}'))[:24]\n",
    "        bz_score = row['blue_zone_score']\n",
    "        life_exp = row['life_expectancy']\n",
    "        is_known_bz = 'Yes' if row.get('is_blue_zone', 0) == 1 else 'No'\n",
    "        \n",
    "        print(f\"{i:<5} {region_name:<25} {bz_score:<10.1f} {life_exp:<12.1f} {is_known_bz:<10}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nScoring Summary:\")\n",
    "    print(f\"Mean Score: {ranked_regions['blue_zone_score'].mean():.1f}\")\n",
    "    print(f\"Standard Deviation: {ranked_regions['blue_zone_score'].std():.1f}\")\n",
    "    print(f\"High Potential Regions (>80): {len(ranked_regions[ranked_regions['blue_zone_score'] > 80])}\")\n",
    "    print(f\"Good Potential Regions (60-80): {len(ranked_regions[(ranked_regions['blue_zone_score'] > 60) & (ranked_regions['blue_zone_score'] <= 80)])}\")\n",
    "    \n",
    "    return ranked_regions\n",
    "\n",
    "# Create Blue Zone scoring system\n",
    "if not df.empty and significant_features:\n",
    "    scored_regions = create_blue_zone_scorer(df, significant_features)\n",
    "else:\n",
    "    scored_regions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Export and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export results if functions and data are available\nimport os\n\noutput_dir = '../outputs'\nos.makedirs(output_dir, exist_ok=True)\nsaved_files = []\n\n# Export scored regions if available\nif 'scored_regions' in locals() and scored_regions is not None:\n    scored_file = os.path.join(output_dir, 'optimized_longevity_analysis.csv')\n    scored_regions.to_csv(scored_file, index=False)\n    saved_files.append(scored_file)\n    print(f\"Scored regions saved to: {scored_file}\")\n\n# Export model results if available\nif 'all_results' in locals() and all_results:\n    model_results_summary = pd.DataFrame({\n        'Model': list(all_results.keys()),\n        'CV_R2_Mean': [r['cv_r2_mean'] for r in all_results.values()],\n        'CV_R2_Std': [r['cv_r2_std'] for r in all_results.values()],\n        'Train_R2': [r['train_r2'] for r in all_results.values()],\n        'MAE': [r['mae'] for r in all_results.values()],\n        'RMSE': [r['rmse'] for r in all_results.values()]\n    })\n    \n    model_file = os.path.join(output_dir, 'model_comparison_results.csv')\n    model_results_summary.to_csv(model_file, index=False)\n    saved_files.append(model_file)\n    print(f\"Model comparison saved to: {model_file}\")\n\n# Export feature importance if available\nif 'importance_data' in locals() and importance_data is not None:\n    importance_file = os.path.join(output_dir, 'feature_importance_analysis.csv')\n    importance_data.to_csv(importance_file, index=False)\n    saved_files.append(importance_file)\n    print(f\"Feature importance saved to: {importance_file}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MODEL OPTIMIZATION ANALYSIS COMPLETE\")\nprint(\"=\" * 80)\nprint(f\"\\nFiles generated: {len(saved_files)}\")\nfor file_path in saved_files:\n    print(f\"  - {os.path.basename(file_path)}\")\n\nprint(f\"\\nAll outputs saved to: ../outputs/\")\nprint(\"Analysis completed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has performed comprehensive model optimization and feature analysis:\n",
    "\n",
    "1. **Systematic Correlation Analysis**: Identified features most strongly associated with longevity\n",
    "2. **Model Comparison**: Evaluated multiple machine learning algorithms with hyperparameter optimization\n",
    "3. **Feature Importance**: Determined which variables are most predictive of life expectancy\n",
    "4. **Actionable Insights**: Categorized features by policy actionability\n",
    "5. **Blue Zone Scoring**: Created a quantitative system for evaluating Blue Zone potential\n",
    "6. **Practical Tools**: Developed a prediction function for life expectancy estimation\n",
    "\n",
    "The analysis provides both scientific insights and practical tools for understanding and predicting longevity patterns globally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}